---
title: "![](uc3m_logo.png) \\ Time Series 1"
author: 
    - "Jialian Zhou He -- 100407485"
    - "José Ignacio Díez Ruíz -- 100487766"
    - "Pablo Vidal Fernández -- 100483812"
    - "Carlos Roldán Piñero -- 100484904"
date: "February 2023"
header-includes:
  - \renewcommand{\and}{\\}
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(fpp2)
library(fpp3)
```

# Exercise 2

> The following time plots and ACF plots correspond to four different
time series.
Your task is to match each time plot in the first row with one
of the ACF plots in the second row.

We will approach this exercise by first giving the chosen match and then
justifying it:

- `1 - C`. The first time series exhibits seasonal pattern which does not
appear to have a predominant period.
This is why we are looking at a corrplot which shows fluctuations in the
magnitudes of the peaks with yet a repeatable pattern.
This is precisely what may be found in `C`.

- `2 - B`. This one exhibits a slight positive tendency.
Nonetheless, the main property of this plot is its heavy seasonal peaks which
should in turn correspond to the heaviest peaks on the ACF plots.
This is why we select `B` as the match, having the most prominent peaks
with a period of 12 months.

- `3 - D`. Once again we have a a slight positive tendency with prominent
seasonal peaks.
Nonetheless, this time the tendency appears to be flatter and the peaks are
less strong overall with respect to `2`.
As such, we select `D` which haves faster decreasing aurocorrelations than
`C` while keeping a clear 12 months seasonality.

- `4 - A`. For the last one, we see tendency on the time series and an
lack of any kind of seasonality.
Hence the lag plot should follow a slowly decrease without periodic
peaks, just what `A` shows.

# Exercise 6

> Use the monthly Australian short-term overseas visitors data,
May 1985-April 2005 (Data set visitors).
>
> (a) With the help of the appropriate graphical representation,
please describe the main features of the series.
>
> (b) Forecast the next two years using Holt-Winters method according
to the features you found in the previous point. Justify your choice.
>
> (c) Experiment with making the trend exponential and/or damped.
Do you see any difference?
Justify your answer.
>
> (d) Now fit each of the following models to the same data:
>    (1) an ETS model
>    (2) an additive ETS model applied to a Box-Cox transformed series
>    (3) an STL decomposition applied to the Box-Cox transformed data
>    followed by an ETS model
>    applied to the seasonally adjusted (transformed) data.
>    Plot all the forecasts together.
>
> (e) For each model, look at the residual diagnostics and compare the
forecasts for the next two years. Which do you prefer?

We take the data `visitors` from the `fpp2` package.
It exhibits the following form:

```{r, fig.cap = "Time series plot of the `visitors` dataset."}
head(visitors, 5 * 12)
autoplot(visitors, ylab = "visitors", xlab = "Year",
         main="Australian short-term overseas visitors") + theme_bw()
```

## (a)

We note a strong seasonality component with no apparent cyclic behavior
which can be more prominently shown in a seasonal plot or a polar plot.
In it we note that there is a yearly period with a clear anual pattern.
There seems to be also a positive tendency, apparently linear.

```{r, fig.cap = "Seasonal plot for the `visitors` dataset."}
ggseasonplot(visitors, ylab = "visitors", xlab = "Year",
             main = "Seasonal plot") + theme_bw()
```

```{r, fig.cap = "Seasonal polar plot for the `visitors` dataset."}
ggseasonplot(visitors, ylab = "visitors", xlab = "Year",
             polar = TRUE, main = "Polar plot") + theme_bw()
```

Moreover, in the corresponding ACF plot we observe a slowly decreasing
autocorrelation, hinting tendency, combined with periodicity in the peaks,
signing mark of seasonality with a 12 month period.

```{r, fig.cap = "Autocorrelations of the `visitor` dataset. Lags are in months."}
ggAcf(visitors) + ggtitle("ACF plot") + theme_bw()
```

Let us plot two decomposition graphs, one additive and another multiplicative,
and then decide which of them better fits our data.

We see the additive one leads to a distribution of the remainder which
does appear to have heteroskedasticity.
More precisely, at the later years there are quite prominent peaks compared
to other time ranges.
In the center there is kind of a dip.

```{r, fig.cap = "Classical decomposition of the `visitors` data set assuming the underlaying model is additive on the tendency and errors."}
autoplot(decompose(visitors, "additive")) + theme_bw()
```

On the other hand, with the multiplicative decomposition we observe a more
even distribution of remainders, which do not look distributed following
any pattern, without clear differences between areas.

```{r, fig.cap = "Classical decomposition of the `visitors` data set assuming the underlaying model is multiplicative on the tendency and errors."}
autoplot(decompose(visitors, "multiplicative")) + theme_bw()
```

With these facts in mind, we decide the latter, the multiplicative model,
better fits our data.

## (b)

As discussed before, we will use a multiplicative seasonal decomposition
when performing a Holter-Winters fit.
We also decide that, for both, performance of the forecast and numerical
cost, we will limit the data used for forecasting from the year 2000
onwards.

```{r, fig.cap = "Forecast of two years using multiplicative Holter-Winters as the the forecasting model."}
vis_cut <- window(visitors, start = 2000)
fit_hw  <- hw(vis_cut, seasonal = "multiplicative", h = 2 * 12)

autoplot(window(vis_cut, start = 1995)) +
    autolayer(fit_hw, PI = FALSE, col = "deepskyblue4") +
    xlab("Year") + ylab("Visitors") + ggtitle("Forecasting 2 years") + theme_bw()
```

We see that the previously made decision does indeed seem grounded as
the forecast, at least on the naked eye, does look as a good candidate.

## (c)

We are now tasked with testing the forecast with different options.
Taking the same Holter-Winters multiplicative model, we experiment
by choosing exponential trend, damped trend and both of them at once.
If we plot them together we see that all of them do *a priori* a good
job in forecasting the following two years.
In this regards, the exponential trend offers more volatile extrema,
while the damped offers the smallest.
The curve with both of them does seem like a middle compromise between
the exponential and damped models.

```{r, fig.cap = "Forecast of two years using multiplicative Holter-Winters as the the forecasting model with different modifications to the tendency component: exponential, damped and both combined."}
fit_exp  <- hw(vis_cut, seasonal = "multiplicative", h = 2 * 12, exponential = TRUE)
fit_dam  <- hw(vis_cut, seasonal = "multiplicative", h = 2 * 12, damped = TRUE)
fit_both <- hw(vis_cut, seasonal = "multiplicative", h = 2 * 12,
               exponential = TRUE, damped = TRUE)

autoplot(window(vis_cut, start = 2000)) +
    autolayer(fit_exp,  PI = FALSE, series = "Exponential") +
    autolayer(fit_dam,  PI = FALSE, series = "Damped") +
    autolayer(fit_both, PI = FALSE, series = "Exp + Damp") +
    xlab("Year") + ylab("Visitors") + ggtitle("Testing HW hyperparameters") +
    guides(colour = guide_legend(title ="Model")) + theme_bw()
```

It seems clear that by eyeballing we will not be able to properly
select one of these models as the most appropiate.
Hence we resort to metrics such as AIC, AICc and BIC.
Looking at them it seems that the previous modifications
do indeed improve on the forecasting.
Among them, the exponential and damped takes the edge over the 
just damped one by a slight margin, giving us an indication that
reduced extrema better forecasts our time series.

```{r}
knitr::kable(data.frame(
    "Default"  = c(fit_hw$model$aic,   fit_hw$model$aicc,   fit_hw$model$bic),
    "Exp"      = c(fit_exp$model$aic,  fit_exp$model$aicc,  fit_exp$model$bic),
    "Damped"   = c(fit_dam$model$aic,  fit_dam$model$aicc,  fit_dam$model$bic),
    "Exp.Damp" = c(fit_both$model$aic, fit_both$model$aicc, fit_both$model$bic),
    row.names = c("AIC", "AICc", "BIC")
))
```

## (d)

This is a just computational part in which are tasked with fitting
three models:

1. An ETS model.
Automatically this chooses the model for the error, trend and seasonality
that better suits our data.
Note that it chose the same that we used for our Holter-Winter model,
giving us confidence on our previously extracted conclusions.

```{r}
fit_ets <- ets(vis_cut)
summary(fit_ets)
```

2. An additive ETS model with Box-Cox transformed data.
Here we impose the additivity of the model but let the algorithm to
determine the correct $\lambda$ needed for the transformation.

```{r}
fit_bc  <- ets(vis_cut, model = "AAA", lambda = "auto")
summary(fit_bc)
```

3. An additive ETS model applied to the STL-seasonality-freed
Box-Cox transformed time series.
Again, we let the algorithm decide the appropriate transformation
parameter.
Nonetheless, we fix the seasonality period as 12 months.

```{r}
fit_stl <- stlm(vis_cut, s.window = 12, method = "ets", lambda = "auto")
summary(fit_stl)
```

## (e)

At last, we are tasked with comparing these last three approaches
to see which of them performs better.
To do so, we first plot all of them together and see whether there
is some significant difference between them visible by the naked eye.

If we look at the forecast, we note that the additive ETS applied
to the Box-Cox transformed data has in general higher values.
Then, when using the seasonality adjust forecast, there is a shift
to lower values while keeping a similar shape.
Lastly, the plain ETS prediction seems to have smaller values in general,
with more prominent minima.

```{r, fig.cap = "Forecast of two years with three different models: ETS (the algorithm chose 'MAM' as the most appropiated decomposition), additive ETS applied to the Box-Cox transformed data, and additive ETS applied to the seasonally adjusted (via STL decomposition) Box-Cox transformed data."}
autoplot(vis_cut) +
    autolayer(forecast::forecast(fit_ets, h = 2 * 12),
              PI = FALSE, series = "ETS") +
    autolayer(forecast::forecast(fit_bc,  h = 2 * 12),
              PI = FALSE, series = "Box-Cox") +
    autolayer(forecast::forecast(fit_stl, h = 2 * 12),
              PI = FALSE, series = "BC-Seas") + 
    xlab("Year") + ylab("Visitors") + ggtitle("Three model comparison") +
    guides(colour = guide_legend(title ="Model")) + theme_bw()
```

Again, just by looking at the graph we cannot tell which of them better
describe our time series.
Thus we resort to accuracy metrics.
More exactly, we will be using the `tsCV` function which performs
cross-validations and return a vector of residuals.
We then sum the square residuals and declare the one with the
smallest sum as the most suitable for our case.
We also plot them to have visual feedback on the magnitude and whether
they resemble white noise, which all of them do.
This happens for the last of the models, which performed
slightly better than the rest, highlighting the vital importance of
taking the necessary preprocessing steps before fitting and forecasting.

```{r}
f1 <- function(y, h) { forecast::forecast(ets(y, model = "MAM"), h = h) }
f2 <- function(y, h) { forecast::forecast(ets(y, model = "AAA",
                                          lambda = "auto"), h = h) }
f3 <- function(y, h) { forecast::forecast(
  stlm(y, s.window = 12, method = "ets", lambda = "auto"), h = h) }
e1 <- tsCV(vis_cut, f1, h = 1)
e2 <- tsCV(vis_cut, f2, h = 1)
e3 <- tsCV(vis_cut, f3, h = 1)

# e3 starts at 2002, we cut them
e1 <- window(e1, start = 2002)
e2 <- window(e2, start = 2002)
e3 <- window(e3, start = 2002)

c(
    "ETS"     = sqrt(mean(e1 ^ 2, na.rm = TRUE)),
    "Box-Cox" = sqrt(mean(e2 ^ 2, na.rm = TRUE)),
    "BC-Seas" = sqrt(mean(e3 ^ 2, na.rm = TRUE))
)
```

```{r, fig.cap = "Cross-Validation computed residuals on the `visitors` dataset from year 2002 onwards with three different models: ETS (the algorithm chose 'MAM' as the most appropiated decomposition), additive ETS applied to the Box-Cox transformed data, and additive ETS applied to the seasonally adjusted (via STL decomposition) Box-Cox transformed data."}
autoplot(e1, series = "ETS") +
    autolayer(e2, series = "Box-Cox") +
    autolayer(e3, series = "BC-Seas") +
    xlab("Year") + ylab("Visitors") + ggtitle("Residuals") +
    guides(colour = guide_legend(title ="Model")) + theme_bw()
```
